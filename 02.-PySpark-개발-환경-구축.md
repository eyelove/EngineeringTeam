# **개요**

- 이 문서는 AWS EC2 서버 1대에 PySpark을 이용해 개발하기 위한 개발 환경 구축 과정을 담은 문서입니다.
- 자세한 설명은 생략하고 최대한 명령어 위주로 담았습니다.
- EC2 운영체제의 버전은 Ubuntu Server 16.04 LTS입니다.
- 이 문서는 [Getting Spark, Python, and Jupyter Notebook running on Amazon EC2](https://medium.com/@josemarcialportilla/getting-spark-python-and-jupyter-notebook-running-on-amazon-ec2-dec599e1c297)를 참고하여 작성되었습니다.

------

# **설치할 소프트웨어**

- Anaconda for Python3
- Java
- Scala
- Apache Hadoop
- Apache Spark
- Apache Hive

------

# **설치**

다음의 단계로 나누어 설치를 진행한다. **01.PySpark 실습 환경 구축, Python 개발 환경 구축**에 이어 진행한다.

1. [AWS 가입 & EC2 인스턴스 생성](https://github.com/YBIGTA/SparkStudy/wiki/01.-AWS-EC2%EC%97%90-PySpark-%EA%B0%9C%EB%B0%9C-%ED%99%98%EA%B2%BD-%EA%B5%AC%EC%B6%95%ED%95%98%EA%B8%B0#1-aws-%EA%B0%80%EC%9E%85--ec2-%EC%9D%B8%EC%8A%A4%ED%84%B4%EC%8A%A4-%EC%83%9D%EC%84%B1)
2. [SSH를 이용해 EC2 인스턴스에 접속](https://github.com/YBIGTA/SparkStudy/wiki/01.-AWS-EC2%EC%97%90-PySpark-%EA%B0%9C%EB%B0%9C-%ED%99%98%EA%B2%BD-%EA%B5%AC%EC%B6%95%ED%95%98%EA%B8%B0#2-ssh%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%B4-ec2-%EC%9D%B8%EC%8A%A4%ED%84%B4%EC%8A%A4%EC%97%90-%EC%A0%91%EC%86%8D)
3. [기본 셋팅](https://github.com/YBIGTA/SparkStudy/wiki/01.-AWS-EC2%EC%97%90-PySpark-%EA%B0%9C%EB%B0%9C-%ED%99%98%EA%B2%BD-%EA%B5%AC%EC%B6%95%ED%95%98%EA%B8%B0#3-%EA%B8%B0%EB%B3%B8-%EC%85%8B%ED%8C%85)
4. [Python 환경 구성](https://github.com/YBIGTA/SparkStudy/wiki/01.-AWS-EC2%EC%97%90-PySpark-%EA%B0%9C%EB%B0%9C-%ED%99%98%EA%B2%BD-%EA%B5%AC%EC%B6%95%ED%95%98%EA%B8%B0#4-python-%ED%99%98%EA%B2%BD-%EA%B5%AC%EC%84%B1)
5. [Java 설치](https://github.com/YBIGTA/SparkStudy/wiki/01.-AWS-EC2%EC%97%90-PySpark-%EA%B0%9C%EB%B0%9C-%ED%99%98%EA%B2%BD-%EA%B5%AC%EC%B6%95%ED%95%98%EA%B8%B0#5-java-%EC%84%A4%EC%B9%98)
6. [Scala 설치](https://github.com/YBIGTA/SparkStudy/wiki/01.-AWS-EC2%EC%97%90-PySpark-%EA%B0%9C%EB%B0%9C-%ED%99%98%EA%B2%BD-%EA%B5%AC%EC%B6%95%ED%95%98%EA%B8%B0#6-scala-%EC%84%A4%EC%B9%98)
7. [Hadoop 설치](https://github.com/YBIGTA/SparkStudy/wiki/01.-AWS-EC2%EC%97%90-PySpark-%EA%B0%9C%EB%B0%9C-%ED%99%98%EA%B2%BD-%EA%B5%AC%EC%B6%95%ED%95%98%EA%B8%B0#7-hadoop-%EC%84%A4%EC%B9%98)
8. [Spark 설치](https://github.com/YBIGTA/SparkStudy/wiki/01.-AWS-EC2%EC%97%90-PySpark-%EA%B0%9C%EB%B0%9C-%ED%99%98%EA%B2%BD-%EA%B5%AC%EC%B6%95%ED%95%98%EA%B8%B0#8-spark-%EC%84%A4%EC%B9%98)
9. [Hive 설치](https://github.com/YBIGTA/SparkStudy/wiki/01.-AWS-EC2%EC%97%90-PySpark-%EA%B0%9C%EB%B0%9C-%ED%99%98%EA%B2%BD-%EA%B5%AC%EC%B6%95%ED%95%98%EA%B8%B0#9-hive-%EC%84%A4%EC%B9%98)

## 5. Java 설치

```bash
# Download JDK8 and install
$ cd $HOME
$ sudo apt-get install openjdk-8-jdk

# ~/.bash_profile 수정
$ vi ~/.bash_profile
```

완성된 ~/.bash_profile의 내용은 아래와 같습니다. 자바의 환경 변수를 추가해 줍니다.

```bash
# Get the aliases and functions
if [ -f ~/.bashrc ]; then
    . ~/.bashrc
fi

# Anaconda
export ANACONDA_HOME="/home/ubuntu/anaconda3"

# Java
export JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64"

# Path
export PATH=${ANACONDA_HOME}/bin:${JAVA_HOME}/bin:$PATH
```

환경 변수를 적용해 줍니다. bash_profile의 내용을 변경한 후에는 항상 다음 명령을 수행해야 합니다.

```bash
$ source ~/.bash_profile
```



## 6. Scala 설치

```bash
# Download scala and install
$ sudo apt-get install scala
```

## 7. Hadoop 설치

하둡을 Pseudo-Distributed 모드로 설치합니다.

### 7-1. 프로토콜 버퍼 설치

```bash
# Install gcc(C complier), g++(C++ complier), make(GNU make utility to maintain groups of programs)
$ sudo apt-get install gcc
$ sudo apt-get install g++
$ sudo apt-get install make

# Install profobuf
$ cd /usr/local
$ sudo wget https://github.com/google/protobuf/releases/download/v2.6.1/protobuf-2.6.1.tar.gz
$ sudo tar xvzf protobuf-2.6.1.tar.gz
$ cd protobuf-2.6.1

$ sudo ./configure
$ sudo make
$ sudo make install
$ sudo ldconfig

$ protoc --version
# libprotoc 2.6.1
```

### 7-2. 하둡 설치

```bash
# Download and install Hadoop - ver. 2.9.0
$ cd $HOME
$ wget http://apache.mirror.cdnetworks.com/hadoop/common/hadoop-2.9.0/hadoop-2.9.0.tar.gz
$ tar xvzf hadoop-2.9.0.tar.gz

# Make symbolic link for hadoop
$ ln -s hadoop-2.9.0 hadoop

# 설치 파일을 다운로드 폴더로 이동합니다
$ mv hadoop-2.9.0.tar.gz ./downloads/
```

### 7-3. 하둡 설정

```bash
# 하둡 설정파일이 있는 경로로 이동합니다

$ cd $HOME/hadoop/etc/hadoop
```

1. hadoop-env.sh 수정

```bash
# 아래의 부분을 찾아 다음과 같이 수정합니다.

# 서버에 JAVA가 있는 위치를 설정합니다. 본인이 설치한 경로에 따라 아래의 경로와 다를 수 있습니다.
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

...

export HADOOP_PID_DIR=/home/ubuntu/hadoop/pids
```

1. masters & slaves 수정
   masters, slaves 파일의 내용이 다음과 같도록 수정합니다.

```bash
localhost
```

1. core-site.xml 수정

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://localhost:9010</value>
  </property>
</configuration>
```

1. hdfs-site.xml 수정

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>/home/ubuntu/data/dfs/namenode</value>
  </property>
  <property>
    <name>dfs.namenode.checkpoint.dir</name>
    <value>/home/ubuntu/data/dfs/namesecondary</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>/home/ubuntu/data/dfs/datanode</value>
  </property>
  <property>
    <name>dfs.http.address</name>
    <value>localhost:50070</value>
  </property>
  <property>
    <name>dfs.secondary.http.address</name>
    <value>localhost:50090</value>
  </property>
</configuration>
```

1. mapred-site.xml 수정

```xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
</configuration>
```

1. yarn-site.xml 수정

```xml
<?xml version="1.0"?>

<configuration>
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>localhost</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
  </property>
  <property>
    <name>yarn.nodemanager.local-dirs</name>
    <value>/home/ubuntu/data/yarn/nm-local-dir</value>
  </property>
  <property>
    <name>yarn.resourcemanager.fs.state-store.uri</name>
    <value>/home/ubuntu/data/yarn/system/rmstore</value>
  </property>
</configuration>
```

1. bash_profile 수정

```bash
# Get the aliases and functions
if [ -f ~/.bashrc ]; then
    . ~/.bashrc
fi

# Anaconda
export ANACONDA_HOME="/home/ubuntu/anaconda3"

# Java
export JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64"

# Hadoop
export HADOOP_HOME="/home/ubuntu/hadoop"

# Path
export PATH=${ANACONDA_HOME}/bin:${JAVA_HOME}/bin:${HADOOP_HOME}/bin:$PATH
```

### 7-4. SSH 설정

하둡에서는 데몬을 실행하기 위해 ssh로 접속할 수 있어야 합니다. 

```bash
$ cd $HOME
$ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

# 아래와 같이 입력해 localhost에 접속할 수 있는지 확인합니다.
$ ssh localhost

# localhost를 종료합니다
$ exit
```

### 7-5. 하둡 실행

```bash
$ cd $HADOOP_HOME

# Namenode format
$ ./bin/hdfs namenode -format

# 데몬의 시작
$ ./sbin/start-dfs.sh
$ ./sbin/start-yarn.sh

# 데몬이 정상적으로 시작되었을 때 실행된 프로세스의 예
$ jps

30817 Jps
30518 NodeManager
29898 DataNode
30092 SecondaryNameNode
30269 ResourceManager
29743 NameNode

# 데몬의 중지
$ ./sbin/stop-yarn.sh
$ ./sbin/stop-dfs.sh
```



## 8. Spark 설치

스파크를 설치합니다.

### 8-1. py4j 설치

```bash
# pip 설치
$ conda install pip

# pip 설치 확인
$ which pip

# py4j 설치
$ pip install py4j
```

### 8-2. spark 설치

```bash
$ cd $HOME

# Download Spark and install - ver. 2.3.0
$ wget http://apache.mirror.cdnetworks.com/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz
$ tar xvzf spark-2.3.0-bin-hadoop2.7.tgz

# Make symbolic link for spark
$ ln -s spark-2.3.0-bin-hadoop2.7 spark

# 설치 파일을 다운로드 폴더로 이동합니다
mv spark-2.3.0-bin-hadoop2.7.tgz ./downloads/
```

### 8-3. spark 설정

1. spark-env.sh 수정

```bash
$ cd $HOME/spark/conf

# spark-env.sh.template을 복사하여 spark-env.sh 생성
$ cp spark-env.sh.template spark-env.sh


# 수정 내용, 아래의 내용을 spark-env.sh 에 덧붙여 줍니다.
export SPARK_MASTER_WEBUI_PORT=9090
export SPARK_WORKER_WEBUI_PORT=9091
export HADOOP_CONF_DIR=/home/ubuntu/hadoop/etc/hadoop
```

1. bash_profile 수정

```bash
# Get the aliases and functions
if [ -f ~/.bashrc ]; then
    . ~/.bashrc
fi

# Anaconda
export ANACONDA_HOME="/home/ubuntu/anaconda3"

# Java
export JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64"

# Hadoop
export HADOOP_HOME="/home/ubuntu/hadoop"

# Spark
export SPARK_HOME="/home/ubuntu/spark"

# PySpark
export PYSPARK_DRIVER_PYTHON=jupyter
export PYSPARK_DRIVER_PYTHON_OPTS='notebook'

# Path
export PATH=${ANACONDA_HOME}/bin:${JAVA_HOME}/bin:${HADOOP_HOME}/bin:${SPARK_HOME}/bin:$PATH
```

## 9. Hive 설치

### 9-1. Hive 설치

아파치 더비를 이용해 Hive 환경을 구축합니다.

```bash
$ cd $HOME

# Download and install hive - ver. 2.3.3
$ wget http://apache.mirror.cdnetworks.com/hive/hive-2.3.3/apache-hive-2.3.3-bin.tar.gz
$ tar xvzf apache-hive-2.3.3-bin.tar.gz

# Make symbolic link for hive
$ ln -s apache-hive-2.3.3-bin hive

# 설치 파일을 다운로드 폴더로 이동합니다
$ mv apache-hive-2.3.3-bin.tar.gz ./downloads/
```

### 9-2. Hive 설정

```bash
$ cd $HOME
$ cd hive/conf
$ cp hive-env.sh.template hive-env.sh
$ vim hive-env.sh
```

hive-env.sh 에 다음과 같이 입력합니다.

```bash
HADOOP_HOME=/home/ubuntu/hadoop
```

hive-site.xml 이 존재하지 않기 때문에 만들고 아래와 같은 내용을 넣어줍니다.

```xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
  <property>
    <name>hive.metastore.warehouse.dir</name>
    <value>/user/hive/warehouse/</value> 
  </property>
  <property>
    <name>hive.cli.print.header</name>
    <value>true</value>
  </property>
  <property>
      <name>hive.metastore.uris</name>
      <value>thrift://localhost:9083</value>
  </property>
  <property>
      <name>hive.metastore.schema.verification</name>
      <value>false</value>
  </property>
</configuration>
```

### 9-3. Hive 실행

```bash
# Hadoop 실행
$ $HADOOP_HOME/sbin/start-dfs.sh
$ $HADOOP_HOME/sbin/start-yarn.sh

# HDFS 디렉토리 생성/권한 부여
$ hdfs dfs -mkdir -p /tmp/hive
$ hdfs dfs -mkdir -p /user/hive/warehouse
$ hdfs dfs -chmod g+w /tmp
$ hdfs dfs -chmod 777 /tmp/hive
$ hdfs dfs -chmod g+w /user/hive
$ hdfs dfs -chmod g+w /user/hive/warehouse

# hive 메타스토어 초기화
$ cd $HOME/hive
$ ./bin/schematool -initSchema -dbType derby
```

### 9-4. Hive 설정파일 Spark conf로 복사

```bash
# Hive 설정 파일을 spark/conf로 복사
$ cp $HOME/hive/conf/hive-site.xml $HOME/spark/conf/
```

### 9-5. Hive metastore 실행

```bash
$ ./bin/hive --service metastore
# 명령어 맨 뒤에 &를 붙이면 백그라운드로 실행됩니다.

# 정상적으로 시작되었을 때 실행된 프로세스의 예
$ jps

20273 NameNode
21316 RunJar
20436 DataNode
20807 ResourceManager
20956 NodeManager
21406 Jps
20639 SecondaryNameNode
```



## 10. PySpark 동작 확인

```bash
$ cd $HOME

# pyspark 실행
$ pyspark
```

각자의 ec2 주소로 jupyter에 접속합니다. http가 아닌 **https**임에 유의합니다.

```bash
https://${ec2-ip-address}:10001
```

새로운 jupyter notebook 파일을 만든 뒤 sc를 입력합니다. sc는 SparkContext의 약자입니다. 다음과 비슷한 결과가 나오면 성공입니다.

```bash
SparkContext
Spark UI
Version
v2.3.0
Master
local[*]
AppName
PySparkShell
```
