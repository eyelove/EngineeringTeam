- 이 문서는 AWS EC2 서버 1대에 PySpark을 이용해 개발하기 위한 개발 환경 구축 과정을 담은 문서입니다.
- 자세한 설명은 생략하고 최대한 명령어 위주로 담았습니다.
- EC2 운영체제의 버전은 Ubuntu Server 16.04 LTS입니다.
- 이 문서는 [Getting Spark, Python, and Jupyter Notebook running on Amazon EC2](https://medium.com/@josemarcialportilla/getting-spark-python-and-jupyter-notebook-running-on-amazon-ec2-dec599e1c297)를 참고하여 작성되었습니다.

***

# **설치할 소프트웨어**
- Anaconda for Python3
- Java
- Scala
- Apache Hadoop
- Apache Spark
- Apache Hive

***
# **설치**
다음의 단계로 나누어 설치를 진행한다.
1. [AWS 가입 & EC2 인스턴스 생성](https://github.com/YBIGTA/SparkStudy/wiki/01.-AWS-EC2%EC%97%90-PySpark-%EA%B0%9C%EB%B0%9C-%ED%99%98%EA%B2%BD-%EA%B5%AC%EC%B6%95%ED%95%98%EA%B8%B0#1-aws-%EA%B0%80%EC%9E%85--ec2-%EC%9D%B8%EC%8A%A4%ED%84%B4%EC%8A%A4-%EC%83%9D%EC%84%B1)
2. [SSH를 이용해 EC2 인스턴스에 접속](https://github.com/YBIGTA/SparkStudy/wiki/01.-AWS-EC2%EC%97%90-PySpark-%EA%B0%9C%EB%B0%9C-%ED%99%98%EA%B2%BD-%EA%B5%AC%EC%B6%95%ED%95%98%EA%B8%B0#2-ssh%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%B4-ec2-%EC%9D%B8%EC%8A%A4%ED%84%B4%EC%8A%A4%EC%97%90-%EC%A0%91%EC%86%8D)
3. [기본 셋팅](https://github.com/YBIGTA/SparkStudy/wiki/01.-AWS-EC2%EC%97%90-PySpark-%EA%B0%9C%EB%B0%9C-%ED%99%98%EA%B2%BD-%EA%B5%AC%EC%B6%95%ED%95%98%EA%B8%B0#3-%EA%B8%B0%EB%B3%B8-%EC%85%8B%ED%8C%85)
4. [Python 환경 구성](https://github.com/YBIGTA/SparkStudy/wiki/01.-AWS-EC2%EC%97%90-PySpark-%EA%B0%9C%EB%B0%9C-%ED%99%98%EA%B2%BD-%EA%B5%AC%EC%B6%95%ED%95%98%EA%B8%B0#4-python-%ED%99%98%EA%B2%BD-%EA%B5%AC%EC%84%B1)
5. [Java 설치](https://github.com/YBIGTA/SparkStudy/wiki/01.-AWS-EC2%EC%97%90-PySpark-%EA%B0%9C%EB%B0%9C-%ED%99%98%EA%B2%BD-%EA%B5%AC%EC%B6%95%ED%95%98%EA%B8%B0#5-java-%EC%84%A4%EC%B9%98)
6. [Scala 설치](https://github.com/YBIGTA/SparkStudy/wiki/01.-AWS-EC2%EC%97%90-PySpark-%EA%B0%9C%EB%B0%9C-%ED%99%98%EA%B2%BD-%EA%B5%AC%EC%B6%95%ED%95%98%EA%B8%B0#6-scala-%EC%84%A4%EC%B9%98)
7. [Hadoop 설치](https://github.com/YBIGTA/SparkStudy/wiki/01.-AWS-EC2%EC%97%90-PySpark-%EA%B0%9C%EB%B0%9C-%ED%99%98%EA%B2%BD-%EA%B5%AC%EC%B6%95%ED%95%98%EA%B8%B0#7-hadoop-%EC%84%A4%EC%B9%98)
8. [Spark 설치](https://github.com/YBIGTA/SparkStudy/wiki/01.-AWS-EC2%EC%97%90-PySpark-%EA%B0%9C%EB%B0%9C-%ED%99%98%EA%B2%BD-%EA%B5%AC%EC%B6%95%ED%95%98%EA%B8%B0#8-spark-%EC%84%A4%EC%B9%98)
9. [Hive 설치](https://github.com/YBIGTA/SparkStudy/wiki/01.-AWS-EC2%EC%97%90-PySpark-%EA%B0%9C%EB%B0%9C-%ED%99%98%EA%B2%BD-%EA%B5%AC%EC%B6%95%ED%95%98%EA%B8%B0#9-hive-%EC%84%A4%EC%B9%98)

## 1. AWS 가입 & EC2 인스턴스 생성
- [EC2 개요](http://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/EC2_GetStarted.html)
- [EC2 인스턴스 생성](https://aws.amazon.com/ko/getting-started/tutorials/launch-a-virtual-machine/)

## 2. SSH를 이용해 EC2 인스턴스에 접속
```bash

# https://aws.amazon.com/ko/getting-started/tutorials/launch-a-virtual-machine/ 에서 참고했습니다.

c. SSH를 사용하여 인스턴스에 연결합니다.
이 사례에서는 사용자 이름이 ec2-user이고, SSH 키는 2단계 파트 d에서 저장한 디렉터리에 보관되어 있으며,
IP 주소는 2단계 파트 f에서 적어두었습니다. 형식은 ssh -i {.pem 파일의 전체 경로} ec2-user@{인스턴스 IP 주소}입니다.

- Windows 사용자: ssh -i 'c:\Users\yourusername\.ssh\MyKeyPair.pem' ec2-user@{IP_Address}
(예: ssh -i 'c:\Users\adamglic\.ssh\MyKeyPair.pem' ec2-user@52.27.212.125)를 입력합니다.

- Mac/Linux 사용자: ssh -i ~/.ssh/MyKeyPair.pem ec2-user@{IP_Address}
(예: ssh -i ~/.ssh/MyKeyPair.pem ec2-user@52.27.212.125)를 입력합니다.

참고: Amazon Linux 이외에 다른 Linux 인스턴스를 시작한 경우, 다른 사용자 이름이 사용될 수 있습니다. 일반적인 사용자 이름에는 ec2-user, root, ubuntu 및 fedora가 있습니다. 로그인 사용자 이름을 알 수 없는 경우, AMI 공급자에게 문의하십시오.

다음과 유사한 응답이 표시됩니다.

The authenticity of host 'ec2-198-51-100-1.compute-1.amazonaws.com (10.254.142.33)' can't be established. RSA key fingerprint is 1f:51:ae:28:df:63:e9:d8:cf:38:5d:87:2d:7b:b8:ca:9f:f5:b1:6f. Are you sure you want to continue connecting (yes/no)?

Yes를 입력하고 Enter 키를 누릅니다.

```
## 3. 기본 셋팅
```bash
$ cd $HOME

# 디렉토리 생성
# meaning of `opt`: https://stackoverflow.com/questions/12649355/what-does-opt-mean-as-in-the-opt-directory-is-it-an-abbreviation
$ mkdir opt

# package manager(apt-get)을 최신화
$ sudo apt-get update

# ssh 관련 설정
$ ssh-keygen -t rsa
$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
```


## 4. Python 환경 구성
### 4-1. Anaconda 설치
```bash
# change directory to install
$ cd $HOME

# archive for anaconda: https://repo.continuum.io/archive/
$ wget https://repo.continuum.io/archive/Anaconda3-4.4.0-Linux-x86_64.sh
$ bash Anaconda3-4.4.0-Linux-x86_64.sh
```

아나콘다를 설치한 이후 환경 변수를 설정해줍니다.
```bash
# vi로 bash_profile를 엽니다. 없으면 만듭니다.
$ vi ~/.bash_profile
```

완성된 ~/.bash_profile의 내용은 아래와 같습니다.
```bash
# Get the aliases and functions
if [ -f ~/.bashrc ]; then
    . ~/.bashrc
fi

## User specific environment and startup programs

# Anaconda
export ANACONDA_HOME="/home/ubuntu/anaconda3"
export PATH=${ANACONDA_HOME}/bin:$PATH
```

### 4-2. Jupyter 셋팅
```bash
$ cd $HOME

# jupyter 설정을 위한 파일을 만들어준다.
$ jupyter notebook --generate-config

#
$ mkdir certs
$ cd certs

#
$ sudo openssl req -x509 -nodes -days 365 -newkey rsa:1024 -keyout mycert.pem -out mycert.pem

```

설정 파일에 내용을 추가해주어야 합니다.

```bash
$ cd ~/.jupyter/

# vi로 설정 파일을 엽니다.
$ vi jupyter_notebook_config.py
```

jupyter_notebook_config.py 파일을 열면 많은 내용이 주석처리 되어있습니다.
아래의 내용을 파일에 추가한 뒤 파일을 닫습니다.

```python
c = get_config()

# Notebook config this is where you saved your pem cert
c.NotebookApp.certfile = u'/home/ubuntu/certs/mycert.pem'
# Run on all IP addresses of your instance
c.NotebookApp.ip = '*'
# Don't open browser by default
c.NotebookApp.open_browser = False
# Fix port to 8888
c.NotebookApp.port = 8888
```

설정을 마친 뒤엔 jupyter를 실행시켜줍니다.

```bash
$ jupyter notebook
```

각자의 ec2 주소로 jupyter에 접속합니다.
http가 아닌 **https**임에 유의합니다.
접속이 되는 것을 확인한 뒤에 쥬피터 서버를 종료합니다.
```bash
https://${ec2-ip-address}:8888
```

## 5. Java 설치
```bash
$ sudo apt-get install openjdk-8-jdk

$ vi ~/.bash_profile
```

완성된 ~/.bash_profile의 내용은 아래와 같습니다.

```bash
# Get the aliases and functions
if [ -f ~/.bashrc ]; then
    . ~/.bashrc
fi

# Anaconda
export ANACONDA_HOME="/home/ubuntu/anaconda3"

# Java
export JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64"

# Path
export PATH=${ANACONDA_HOME}/bin:${JAVA_HOME}/bin:$PATH
```
## 6. Scala 설치
```bash
$ sudo apt-get install scala
```

## 7. Hadoop 설치
하둡을 Pseudo-Distributed 모드로 설치합니다.

### 7-1. 프로토콜 버퍼 설치
```bash
$ sudo apt-get install gcc
$ sudo apt-get install g++
$ sudo apt-get install make

$ cd /usr/local
$ sudo wget https://github.com/google/protobuf/releases/download/v2.5.0/protobuf-2.5.0.tar.gz
$ sudo tar xvzf protobuf-2.5.0.tar.gz
$ cd protobuf-2.5.0

$ sudo ./configure
$ sudo make
$ sudo make install
$ sudo ldconfig

$ protoc --version
```

### 7-2. 하둡 설치
```bash
$ cd $HOME
$ wget http://apache.mirror.cdnetworks.com/hadoop/common/hadoop-2.7.4/hadoop-2.7.4.tar.gz
$ tar xvzf hadoop-2.7.4.tar.gz
$ ln -s hadoop-2.7.4 hadoop
```

### 7-3. 하둡 설정
```bash
# 하둡 설정파일이 있는 경로로 이동합니다

$ cd $HOME/hadoop/etc/hadoop
```

1. hadoop-env.sh 수정
```bash
# 아래의 부분을 찾아 다음과 같이 수정합니다.

# 서버에 JAVA가 있는 위치를 설정합니다. 본인이 설치한 경로에 따라 아래의 경로와 다를 수 있습니다.
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

...

export HADOOP_PID_DIR=/home/ubuntu/hadoop/pids
```

2. masters & slaves 수정
masters, slaves 파일의 내용이 다음과 같도록 수정합니다.
```bash
localhost
```

3. core-site.xml 수정
```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://localhost:9010</value>
  </property>
</configuration>
```

4. hdfs-site.xml 수정
```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>/home/ubuntu/data/dfs/namenode</value>
  </property>
  <property>
    <name>dfs.namenode.checkpoint.dir</name>
    <value>/home/ubuntu/data/dfs/namesecondary</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>/home/ubuntu/data/dfs/datanode</value>
  </property>
  <property>
    <name>dfs.http.address</name>
    <value>localhost:50070</value>
  </property>
  <property>
    <name>dfs.secondary.http.address</name>
    <value>localhost:50090</value>
  </property>
</configuration>
```

5. mapred-site.xml 수정
```xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
</configuration>
```

6. yarn-site.xml 수정
```xml
<?xml version="1.0"?>

<configuration>
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>localhost</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
  </property>
  <property>
    <name>yarn.nodemanager.local-dirs</name>
    <value>/home/ubuntu/data/yarn/nm-local-dir</value>
  </property>
  <property>
    <name>yarn.resourcemanager.fs.state-store.uri</name>
    <value>/home/ubuntu/data/yarn/system/rmstore</value>
  </property>
</configuration>
```

7. bash_profile 수정
```bash
# Get the aliases and functions
if [ -f ~/.bashrc ]; then
    . ~/.bashrc
fi

# Anaconda
export ANACONDA_HOME="/home/ubuntu/anaconda3"

# Java
export JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64"

# Hadoop
export HADOOP_HOME="/home/ubuntu/hadoop"

# Path
export PATH=${ANACONDA_HOME}/bin:${JAVA_HOME}/bin:${HADOOP_HOME}/bin:$PATH
```

### 7-4. 하둡 실행

```bash
$ cd $HADOOP_HOME

$ ./bin/hdfs namenode -format

# 하둡 시작
$ ./sbin/start-all.sh

# 하둡 종료
$ ./sbin/stop-all.sh
```

## 8. Spark 설치
스파크를 설치합니다.

### 8-1. py4j 설치
```bash
# pip 설치
$ conda install pip

# pip 설치 확인
$ which pip

# py4j 설치
$ pip install py4j
```

### 8-2. spark 설치
```bash
$ cd $HOME
$ wget http://mirror.apache-kr.org/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz
$ tar xvzf spark-2.2.0-bin-hadoop2.7.tgz
$ ln -s spark-2.2.0-bin-hadoop2.7 spark
```

### 8-3. spark 설정

1. spark-env.sh 수정
```bash
$ cd $HOME/spark/conf

# spark-env.sh.template을 복사하여 spark-env.sh 생성
$ cp spark-env.sh.template spark-env.sh


# 수정 내용, 아래의 내용을 spark-env.sh 에 덧붙여 줍니다.
export SPARK_MASTER_WEBUI_PORT=9090
export SPARK_WORKER_WEBUI_PORT=9091
export HADOOP_CONF_DIR=/home/ubuntu/hadoop/etc/hadoop
```

2. bash_profile 수정
```bash
# Get the aliases and functions
if [ -f ~/.bashrc ]; then
    . ~/.bashrc
fi

# Anaconda
export ANACONDA_HOME="/home/ubuntu/anaconda3"

# Java
export JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64"

# Hadoop
export HADOOP_HOME="/home/ubuntu/hadoop"

# Spark
export SPARK_HOME="/home/ubuntu/spark"

# PySpark
export PYSPARK_DRIVER_PYTHON=jupyter
export PYSPARK_DRIVER_PYTHON_OPTS='notebook'

# Path
export PATH=${ANACONDA_HOME}/bin:${JAVA_HOME}/bin:${HADOOP_HOME}/bin:${SPARK_HOME}/bin:$PATH
```

## 9. Hive 설치

### 9-1. Hive 설치

아파치 더비를 이용해 Hive 환경을 구축합니다.

```bash
$ wget http://apache.mirror.cdnetworks.com/hive/hive-2.3.2/apache-hive-2.3.2-bin.tar.gz
$ tar xvzf apache-hive-2.3.0-bin.tar.gz
$ ln -s apache-hive-2.3.0-bin hive

```

### 9-2. Hive 설정
```bash
$ cd $HOME
$ cd hive/conf
$ cp hive-env.sh.template hive-env.sh
$ vi hive-env.sh

```

hive-env.sh 에 다음과 같이 입력합니다.
```bash
HADOOP_HOME=/home/ubuntu/hadoop
```

hive-site.xml 이 존재하지 않기 때문에 만들고 아래와 같은 내용을 넣어줍니다.
```xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
  <property>
    <name>hive.metastore.warehouse.dir</name>
    <value>/user/hive/warehouse/</value>
  </property>
  <property>
    <name>hive.cli.print.header</name>
    <value>true</value>
  </property>
  <property>
      <name>hive.metastore.uris</name>
      <value>thrift://localhost:9083</value>
  </property>
  <property>
      <name>hive.metastore.schema.verification</name>
      <value>false</value>
  </property>
</configuration>
```

### 9-3. Hive 실행

```bash
# Hadoop 실행
$ $HADOOP_HOME/sbin/start-all.sh

# HDFS 디렉토리 생성/권한 부여
$ hdfs dfs -mkdir -p /tmp/hive
$ hdfs dfs -mkdir -p /user/hive/warehouse
$ hdfs dfs -chmod g+w /tmp
$ hdfs dfs -chmod 777 /tmp/hive
$ hdfs dfs -chmod g+w /user/hive
$ hdfs dfs -chmod g+w /user/hive/warehouse

# hive 메타스토어 초기화
$ cd $HOME/hive
$ ./bin/schematool -initSchema -dbType derby
```

### 9-4. Hive 설정파일 Spark conf로 복사
```bash
# Hive 설정 파일을 spark/conf로 복사
$ cp $HOME/hive/conf/hive-site.xml $HOME/spark/conf/
```

### 9-5. Hive metastore 실행
```bash
$ ./bin/hive --service metastore
```
